# metaGTx_processing.R
# This is a template for processing shotgun metagenomic/metatranscriptomic sequences
# using tools from the BioBakery ( https://github.com/biobakery/biobakery )
#
# make sure you have kneaddata and humann installed on your system, can be done with pip

library(metaGTx.processing)
library(magrittr)
library(stringr)

###### MODIFY THESE VARIABLES AS NEEDED
###### VARIABLES YOU DON'T NEED/WANT AT ALL SET TO NULL
###### see ?create.processing.env for help
run.env <- create.processing.env(
  interactive = FALSE,
  job.queue = "darwin@papilio",
  jobs.dir = "/home/micro/stagamak/Jobs",
  max.cores = 100,
  max.concurrent.jobs = 4,
  base.dir = "/home/micro/stagamak/myData2/ZZ_oral_metagenomes/TEST",
  raw.seq.dir = "/home/micro/stagamak/prod/prod_restructure/projects/stagamak/ZZ_oral_metagenomes/FastQs/Run2",
  link.dir = file.path(base.dir, "FASTQs"),
  store.dir = "/home/micro/stagamak/prod/prod_restructure/projects/stagamak/ZZ_oral_metagenomes/TEST",
  temp.dir = file.path(base.dir, "Results"),
  max.memory = "50G",
  qsub.options = "'-m ae -M stagamak@oregonstate.edu'",
  knead.host.db = "/nfs3/Sharpton_Lab/public_databases/Homo_sapiens_hg37_and_human_contamination_Bowtie2_v0.1/hg37dec_v0.1",
  save.env.dir = "Run_envs"
)
# Alternatively
# run.env <- readRDS("PATH/TO/SAVED/ENVIRONMENT")

###### THESE VARIABLES INHERIT FROM ABOVE
concurrent.jobs <- ifelse(run.env$interactive, 1, run.env$max.concurrent.jobs)
cores.per.job <- floor(run.env$max.cores / concurrent.jobs)
for (dir in c(run.env$link.dir, run.env$temp.dir, run.env$store.dir)) {
  if (!dir.exists(dir)) { dir.create(dir) }
}

###### BEGIN PROCESSING
setwd(run.env$link.dir)
symlinkFastqs(
  fastq.dir = run.env$raw.seq.dir,
  delim = "-",
  sample.field = 2,
  pattern = "lane1-s13[0123]"
)
setwd(base.dir)

###### KNEADDATA
knead.output <- file.path(run.env$store.dir, "KneadData_output") # path to store QC'd sequences
knead.other <- file.path(run.env$store.dir, "KneadData_other") # path for other kneaddata output files
knead.array.file <- file.path(run.env$jobs.dir, "kneaddata_array_commands.txt")
for (dir in c(knead.output, knead.other)) {
  if (!dir.exists(dir)) { dir.create(dir) }
}
commands <- generate.full.commands(
  tool = "kneaddata",
  paired = TRUE,
  input.dir = run.env$link.dir,
  output.dir = run.env$store.dir,
  tmp.dir = run.env$temp.dir,
  reference.db = run.env$knead.host.db,
  threads = cores.per.job,
  zip.output = TRUE,
  split = "--",
  sample.field = 1,
  write.to = NULL
)
head(commands) # inspect commands for accuracy
if (run.env$interactive) {
  execute.commands(commands)
} else {
  generate.SGE.commmand(
    c = knead.array.file,
    q = run.env$job.queue,
    P = cores.per.job,
    b = concurrent.jobs,
    f = run.env$max.memory,
    qsub_options = run.env$qsub.options
  )
  ## Submit job with printed command from appropriate machine
}
move.files(
  move.from = run.env$store.dir,
  move.to = knead.output,
  match.pattern = "_paired_[12].fastq.gz"
  )
move.files(
  move.from = run.env$store.dir,
  move.to = knead.other,
  match.pattern = "\\.gz$"
)

###### HUMANN
setwd(run.env$link.dir)
file.remove(list.files())
symlinkFastqs(
  fastq.dir = knead.output,
  delim = "--",
  sample.field = 1,
  pattern = "_1.fastq.gz"
)
setwd(run.env$base.dir)

humann.output <- file.path(run.env$store.dir, "Humann_output")
humann.array.file <- file.path(run.env$jobs.dir, "humann_array_commands.txt")

if (!dir.exists(humann.output)) { dir.create(humann.output) }

### If need to install full knead database before running humann. Run the following
# install.chocophlan("/home/micro/stagamak/Sharpton_NFS/public_databases")

commands <- generate.full.commands(
  tool = "humann",
  paired = FALSE,
  input.dir = run.env$link.dir,
  output.dir = humann.output,
  tmp.dir = run.env$temp.dir,
  threads = cores.per.job,
  write.to = humann.array.file
)
head(commands) # inspect commands for accuracy
if (run.env$interactive) {
  execute.commands(commands)
} else {
  generate.SGE.commmand(
    c = humann.array.file,
    q = run.env$job.queue,
    P = cores.per.job,
    b = concurrent.jobs,
    f = run.env$max.memory,
    qsub_options = run.env$qsub.options
  )
  ## Submit job with printed command from appropriate machine
}

cd $HUMANN_OUTPUT

OUTFILE=percent_unaligned.csv ###
touch $OUTFILE
echo "Sample,Type,Pct.unaligned" >> $OUTFILE

for TGZ in *tgz; do
NAME=`echo $TGZ | cut -d '.' -f 1`
SMPL=`echo $NAME | cut -d '-' -f 1`
echo $SMPL
LOG_PATH=`tar -zvtf $TGZ --wildcards --no-anchored '*.log' | cut -d ' ' -f 6`
LOG_TOPDIR=`dirname $LOG_PATH | cut -d '/' -f 1`
tar -zvxf $TGZ --wildcards --no-anchored '*.log'

NUCL_UNAGLINED=`grep "Unaligned reads after nucleotide alignment" $LOG_PATH | grep -o "[0-9][0-9\.]* %" | sed 's/ %//'`
echo "${SMPL},nucleotide,${NUCL_UNAGLINED}" >> $OUTFILE
PROT_UNAGLINED=`grep "Unaligned reads after translated alignment" $LOG_PATH | grep -o "[0-9][0-9\.]* %" | sed 's/ %//'`
echo "${SMPL},translated,${PROT_UNAGLINED}" >> $OUTFILE

rm -rfv ./$LOG_TOPDIR
done
